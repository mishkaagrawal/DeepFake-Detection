{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9MjABRWn7gul",
    "outputId": "068eee7f-28a7-4bdb-efe1-6a83e7494c8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming download from 1071644672 bytes (2952911046 bytes left)...\n",
      "Resuming download from https://www.kaggle.com/api/v1/datasets/download/xhlulu/140k-real-and-fake-faces?dataset_version_number=2 (1071644672/4024555718) bytes left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3.75G/3.75G [03:24<00:00, 14.4MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/raphaeldamasceno/modelos?dataset_version_number=4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 507M/507M [00:34<00:00, 15.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data source import complete.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
    "# THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# NOTEBOOK.\n",
    "import kagglehub\n",
    "data = kagglehub.dataset_download('xhlulu/140k-real-and-fake-faces')\n",
    "model = kagglehub.dataset_download('raphaeldamasceno/modelos')\n",
    "\n",
    "print('\\nData source import complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Dd02KvydCEp4",
    "outputId": "2091bda6-4999-442e-c0f5-1ec4d717fbfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Mishka\\\\.cache\\\\kagglehub\\\\datasets\\\\xhlulu\\\\140k-real-and-fake-faces\\\\versions\\\\2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-0XEKvPvMolk",
    "outputId": "2aaccd73-5e28-4b1f-e8c8-d58a2df6d7c2"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "modelDownloaded = kagglehub.dataset_download('raphaeldamasceno/modelos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ORlv8gZ27guo"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDN70B6Rv_8M",
    "outputId": "6bba7f74-1b36-4b40-b079-f363028d5029"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting timm\n",
      "  Downloading timm-1.0.15-py3-none-any.whl.metadata (52 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\mishka\\appdata\\roaming\\python\\python312\\site-packages (from timm) (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\mishka\\appdata\\roaming\\python\\python312\\site-packages (from timm) (0.21.0)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from timm) (6.0.1)\n",
      "Collecting huggingface_hub (from timm)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors (from timm)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (24.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->timm) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->timm) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\mishka\\appdata\\roaming\\python\\python312\\site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision->timm) (10.4.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch->timm) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (2024.12.14)\n",
      "Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.4 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 2.5 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Installing collected packages: safetensors, huggingface_hub, timm\n",
      "Successfully installed huggingface_hub-0.30.2 safetensors-0.5.3 timm-1.0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script huggingface-cli.exe is installed in 'C:\\Users\\Mishka\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas matplotlib pillow seaborn torch torchvision tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'app.py',\n",
       " 'project',\n",
       " 'SEAI_Project.ipynb',\n",
       " '__pycache__']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Mishka\\\\OneDrive\\\\Desktop\\\\seai'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "Zxde3YYs7guo",
    "outputId": "73610868-c17f-42f3-e76d-730235b210bf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import seaborn as sb\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n",
    "from torchvision.utils import make_grid\n",
    "from torch.autograd import Variable\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support\n",
    "import shutil  ##offers high-level operation on a file like a copy, create, and remote operation on the file\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "code_dir = \"/project/working/code\"\n",
    "model_dir = \"/project/working/model\"\n",
    "output_dir = \"/project/working/output\"\n",
    "\n",
    "if not os.path.exists(code_dir):\n",
    "    os.makedirs(code_dir)\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "shutil.copyfile(src=os.path.join(modelDownloaded, \"convnext.py\"),\n",
    "                dst=\"/project/working/code/convnext.py\")\n",
    "shutil.copyfile(src=os.path.join(modelDownloaded,\"convnext_tiny_1k_224_ema.pth\"),\n",
    "                dst=\"/project/working/model/convnext_tiny_1k_224_ema.pth\")\n",
    "\n",
    "os.chdir(\"/project/working/code\")\n",
    "\n",
    "\n",
    "from convnext import ConvNeXt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nymuChf77gup"
   },
   "source": [
    "# **2. Database**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PDWfmfx7gup"
   },
   "source": [
    "## **2.1. Network Architectures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "vUGv-RMgKkYu",
    "outputId": "0b7a8513-bbad-4e7b-ba74-b60d9589aa3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\project\\\\working\\\\code'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "1vmsRnxR7guq"
   },
   "outputs": [],
   "source": [
    "#Instantiating the ConvNeXt model\n",
    "from convnext import ConvNeXt\n",
    "def ConvNeXt_model():\n",
    "    model_conv=ConvNeXt()\n",
    "    state_dict = torch.load('/project/working/model/convnext_tiny_1k_224_ema.pth')\n",
    "    model_conv.load_state_dict(state_dict[\"model\"])\n",
    "\n",
    "    return model_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4E0xbQBG7guq",
    "outputId": "3ae209c0-c196-4eb4-9b7f-96e25aef7362"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "aIbNwswG7gur"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Cpd-2gE47gur"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "F4-is31P7gur"
   },
   "outputs": [],
   "source": [
    "train_acc, test_acc, val_loss, val_loss = [], [], [], []\n",
    "train_precision, val_precision, train_recall, val_recall = [], [], [], []\n",
    "train_f1, val_f1 = [], []\n",
    "df = pd.DataFrame(columns=['Epoch', 'Train ACC', 'Train Loss', 'Train F1', 'Val ACC', 'Val Loss', 'Val F1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwSi-A4b7gus"
   },
   "source": [
    "# **3. Methodology**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQAcafyw7gus"
   },
   "source": [
    "## **3.1.  Data Augmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAcQqf7V7gus"
   },
   "source": [
    "### **3.1.1. Image Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "wEfdClm07gus"
   },
   "outputs": [],
   "source": [
    "def full_data_transform(data_fraction, batch_size):\n",
    "    # Data transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "    local_arquivos=\"C:/Users/Mishka/.cache/kagglehub/datasets/xhlulu/140k-real-and-fake-faces/versions/2/real_vs_fake/real-vs-fake\"\n",
    "    full_train_dataset = ImageFolder(local_arquivos + \"/train\", transform=transform)\n",
    "    full_val_dataset = ImageFolder(local_arquivos + \"/valid\", transform=transform)\n",
    "\n",
    "    # Determine the number of objects to be selected\n",
    "    num_train_data = int(len(full_train_dataset) * data_fraction)\n",
    "    num_val_data = int(len(full_val_dataset) * data_fraction)\n",
    "\n",
    "\n",
    "\n",
    "    # Randomly select objects for the datasets\n",
    "    train_indices = random.sample(range(len(full_train_dataset)), num_train_data)\n",
    "    val_indices = random.sample(range(len(full_val_dataset)), num_val_data)\n",
    "\n",
    "    # Create datasets with randomly selected objects\n",
    "    train_dataset = torch.utils.data.Subset(full_train_dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(full_val_dataset, val_indices)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=int(batch_size/2), shuffle=False)\n",
    "\n",
    "\n",
    "    return train_dataloader, val_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwx_3git7gut"
   },
   "source": [
    "## **3.2. Training and Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "6FNpvodS7gut"
   },
   "outputs": [],
   "source": [
    " # Generic training function\n",
    "def train(model, dataloader, criterion, optimizer, scheduler, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true,y_pred=[], []\n",
    "\n",
    "    loop = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for batch_idx, (images, labels) in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() #Clears the gradients of all optimized tensors before performing backpropagation.\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predicted = outputs.argmax(dim = 1)\n",
    "\n",
    "        y_true.extend(labels.cpu().tolist())\n",
    "        y_pred.extend(predicted.cpu().tolist())\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        loop.set_description(f\"[Epoch {(epoch+1)}]\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "\n",
    "    train_loss = running_loss / len(dataloader.dataset)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro',zero_division=0)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.6f} | Train Accuracy: {(accuracy * 100):.2f}% | Train F1-Score: {f1:.6f}\")\n",
    "\n",
    "    model_name= f'convnext.pth'\n",
    "    torch.save(model.state_dict(), os.path.join('/project/working/model', model_name))\n",
    "\n",
    "    return train_loss, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "UxYPJ0Wf7gut"
   },
   "outputs": [],
   "source": [
    "# Generic Validation function\n",
    "def val(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_pred, y_true= [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            predicted = outputs.argmax(dim = 1)\n",
    "\n",
    "            y_true.extend(labels.cpu().tolist())\n",
    "            y_pred.extend(predicted.cpu().tolist())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss = running_loss / len(list(dataloader.dataset))\n",
    "    val_accuracy = accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, val_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro',zero_division=0)\n",
    "\n",
    "    print(f\"Val Loss: {val_loss:.6f} | Val Accuracy: {(val_accuracy * 100):.2f}% | Val F1-Score: {val_f1:.6f}\")\n",
    "    return val_loss, val_accuracy, val_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "rpkwqJCx7gut"
   },
   "outputs": [],
   "source": [
    "#Generic function for training and testing\n",
    "def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, device, num_epochs):\n",
    "    model=model.to(device)\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('----------------------------------------------------------------------------')\n",
    "        train_loss, train_accuracy, train_f1 = train(model, train_dataloader, criterion, optimizer, scheduler, device, epoch)\n",
    "        val_loss, val_accuracy,test_f1 = val(model, val_dataloader, criterion, device)\n",
    "        df.loc[epoch+1]=[epoch+1, train_accuracy, train_loss, train_f1, val_accuracy, val_loss, val_f1]\n",
    "        df.to_csv('metrics.csv', index = False)\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "    return train_loss, train_accuracy, train_f1, val_loss, val_accuracy, val_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efticPxt7gut"
   },
   "source": [
    "## **3.3. Experimental Protocol**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5CCV0S97gut"
   },
   "source": [
    "Applying experimental protocol to scenarios resulting from permutations of Data Augmentations (AutoAugment and RandAugment) and Fine-Tuning:\n",
    "1. Without Fine-Tuning\n",
    "2. With Fine-Tuning\n",
    "3. Without Fine-Tuning and with AutoAugment\n",
    "4. With Fine-Tuning and with AutoAugment\n",
    "5. Without Fine-Tuning and with RandAugment\n",
    "6. With Fine-Tuning and with RandAugment\n",
    "7. Without Fine-Tuning and with both AutoAugment and RandAugment\n",
    "8. With Fine-Tuning and with both AutoAugment and RandAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JthFZOh_8w0k",
    "outputId": "40259b02-7c31-495a-8803-a7369f0274ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training objects: 10000\n",
      "Number of test objects: 2000\n",
      "============================================================================\n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1]:  12%|██████▍                                                 | 36/313 [10:34<1:21:25, 17.64s/it, loss=0.666]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m     27\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m train_loss, train_accuracy, train_f1, val_loss, val_accuracy, val_f1 \u001b[38;5;241m=\u001b[39m train_model(model, train_dataloader, test_dataloader,\n\u001b[0;32m     30\u001b[0m                                                                             criterion, optimizer, scheduler, device, num_epochs)\n",
      "Cell \u001b[1;32mIn[35], line 11\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, device, num_epochs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----------------------------------------------------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m     train_loss, train_accuracy, train_f1 \u001b[38;5;241m=\u001b[39m train(model, train_dataloader, criterion, optimizer, scheduler, device, epoch)\n\u001b[0;32m     12\u001b[0m     val_loss, val_accuracy,test_f1 \u001b[38;5;241m=\u001b[39m val(model, val_dataloader, criterion, device)\n\u001b[0;32m     13\u001b[0m     df\u001b[38;5;241m.\u001b[39mloc[epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m=\u001b[39m[epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, train_accuracy, train_loss, train_f1, val_accuracy, val_loss, val_f1]\n",
      "Cell \u001b[1;32mIn[33], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, criterion, optimizer, scheduler, device, epoch)\u001b[0m\n\u001b[0;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     19\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_fraction = 0.1\n",
    "num_epochs=5\n",
    "batch_size=32\n",
    "learning_rate=0.001\n",
    "num_classes = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_train_data = int((100000) * data_fraction)\n",
    "num_val_data = int((20000) * data_fraction)\n",
    "\n",
    "train_indices = random.sample(range(65000), num_train_data)\n",
    "val_indices = random.sample(range(20000), num_val_data)\n",
    "\n",
    "print(f'Number of training objects: {num_train_data}')\n",
    "print(f'Number of test objects: {num_val_data}')\n",
    "print(\"============================================================================\")\n",
    "\n",
    "train_dataloader, test_dataloader= full_data_transform(data_fraction, batch_size)\n",
    "model = ConvNeXt_model()\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)  # Replace num_classes with the correct number of classes\n",
    "num=2\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "train_loss, train_accuracy, train_f1, val_loss, val_accuracy, val_f1 = train_model(model, train_dataloader, test_dataloader,\n",
    "                                                                            criterion, optimizer, scheduler, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Et5zRkZ-7guu",
    "outputId": "01274029-792a-4859-e86b-2c239d541fa4"
   },
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T17:32:57.062496Z",
     "iopub.status.busy": "2025-04-09T17:32:57.062151Z",
     "iopub.status.idle": "2025-04-09T17:32:57.06828Z",
     "shell.execute_reply": "2025-04-09T17:32:57.067423Z",
     "shell.execute_reply.started": "2025-04-09T17:32:57.062462Z"
    },
    "id": "rN4aKKLf7guu"
   },
   "outputs": [],
   "source": [
    "df.to_csv('metrics.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 501529,
     "sourceId": 939937,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3382782,
     "sourceId": 6037307,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 140439859,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 34,
     "modelInstanceId": 381,
     "sourceId": 504,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
